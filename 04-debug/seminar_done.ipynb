{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отладка моделей\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m12sl/dl-hse-2021/blob/master/04-debug/seminar.ipynb)\n",
    "\n",
    "\n",
    "План семинара:\n",
    "\n",
    "- [ ] Освоить LR scheduling\n",
    "- [ ] Написать LR range test\n",
    "- [ ] Разобраться с подсчетом валидационных и тренировочных метрик \n",
    "- [ ] Добавим логгирование норм градиентов\n",
    "- [ ] Посмотрим на forward-hook\n",
    "- [ ] Classier Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Scheduling\n",
    "\n",
    "Два типа расписаний:\n",
    "\n",
    "- по эпохам (StepLR, ReduceLROnPlateau, ...) \n",
    "    ```\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    for epoch in range(epochs):\n",
    "        train(...)\n",
    "        validate(...)\n",
    "        scheduler.step()\n",
    "    ```\n",
    "\n",
    "\n",
    "- по батчам (Cosine, Cyclic, 1cycle, ...)\n",
    "    ```\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
    "    for epoch in range(epochs):\n",
    "        # train(...)\n",
    "        for batch in data_loader:\n",
    "            train_batch(...)\n",
    "            scheduler.step()\n",
    "        # validate(...)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор оптимального LR\n",
    "\n",
    "\n",
    "Для выбора оптимального LR удобно использовать т.н. Learning Rate Range Test, часто процедуру называют просто find_lr. Под капотом проход по тренировочной эпохе с lr, изменяемым на каждом батче по формуле:\n",
    "\n",
    "$$\n",
    "\\mathrm{it} = \\frac{\\mathrm{step}}{\\mathrm{total steps}}\\\\\n",
    "\\mathrm{lr} = \\exp\\left\\{ \n",
    "    (1 - t ) \\log a + t \\log b\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "Чтобы поменять LR для всех оптимизируемых параметров, можно пройтись по ним циклом:\n",
    "\n",
    "```\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/02/lr_finder.png\"/>\n",
    "\n",
    "_картинка из бложика [Jeremy Jordan](https://www.jeremyjordan.me/nn-learning-rate/)_\n",
    "\n",
    "\n",
    "Идея приема простая: пока LR меньше некоторого порога на каждом шаге градиентного спуска веса просто не меняются (в частности из-за особенностей операций с плавающей точкой).\n",
    "При очень большом LR мы шагаем слишком далеко и уходим от точки экстремума. \n",
    "\n",
    "Оптимальный LR лежит где-то между ними. Экспоненциальная формула изменения LR позволяет с должным качеством найти хорошую точку.\n",
    "\n",
    "\n",
    "\n",
    "Если интересно: [статья , в которой эту технику предложили и активно использовали](https://arxiv.org/pdf/1506.01186.pdf).\n",
    "\n",
    "\n",
    "**Some math notes**\n",
    "\n",
    "У типов данных с плавающей точкой есть арифметические особенности:\n",
    "\n",
    "$$\n",
    "# fp32\n",
    "x + \\delta == x,\\,\\mathrm{если}\\; \\delta < 5.96 \\cdot 10^{-8} x\n",
    "$$\n",
    "\n",
    "К слову, это еще одна причина присматривать за величинами активаций, нормировать данные и таргет в случае регрессии. Можно было бы перейти на float64, но (вычислительно и по памяти) дешевле быть аккуратными на float32.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://blogs.nvidia.com/wp-content/uploads/2020/05/tf32-Mantissa-chart-hi-res-FINAL-400x255.png.webp\"/>\n",
    "\n",
    "_картинка из статьи [NVIDIA](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики\n",
    "\n",
    "TL; DR:\n",
    "- тренировочные метрики записывать без сглаживания с каждого батча\n",
    "- валидационные собирать за всю валидацию и рисовать одной точкой\n",
    "\n",
    "\n",
    "**Особенности TB**:\n",
    "\n",
    "- При отображении прореживает точки по global_step\n",
    "- Чтобы рисовать на одном графике надо писать в разные папки (завести отдельные train_ и val_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обновим Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision.datasets import FashionMNIST\n",
    "mnist = FashionMNIST(\"./tmp\", train=True, download=True)\n",
    "\n",
    "\n",
    "class VectorSet:\n",
    "    def __init__(self, train=True):\n",
    "        self.data = FashionMNIST(\"./tmp\", train=train, download=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        img, label = self.data[item]\n",
    "        img = np.array(img).astype(np.float32).reshape(-1) / 255.0\n",
    "        return dict(\n",
    "            sample=img,\n",
    "            label=label,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeryModel(nn.Module):\n",
    "    def __init__(self, lr_scheduler=None, lr_scheduler_type=None):\n",
    "        super().__init__()\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.lr_scheduler_type = lr_scheduler_type\n",
    "        if lr_scheduler_type not in [None, 'per_batch', 'per_epoch']:\n",
    "            raise ValueError(\"lr_scheduler_type must be one of: None, 'per_batch', 'per_epoch'. \"\n",
    "                             f\"Not: {lr_scheduler_type}\")\n",
    "\n",
    "        self.inner = nn.Sequential(nn.Linear(784, 100),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(100, 10))\n",
    "        \n",
    "        net = resnet34(pretrained=True)\n",
    "        net.fc = Linear(...)\n",
    "        self.inner = net\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inner(x)\n",
    "\n",
    "    def compute_all(self, batch):  # удобно сделать функцию, в которой вычисляется лосс по пришедшему батчу\n",
    "        x = batch['sample']\n",
    "        y = batch['label']\n",
    "        logits = self.inner(x)\n",
    "        \n",
    "        F.softmax(logits).argmax() == logits.argmax()\n",
    "\n",
    "        loss = F.cross_entropy(logits, y) # весь выч граф\n",
    "        acc = (logits.argmax(axis=1) == y).float().mean().cpu().numpy()\n",
    "        metrics = dict(acc=acc, loss=loss.item())\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "    def post_train_batch(self):\n",
    "        # called after every train batch\n",
    "        if self.lr_scheduler is not None and self.lr_scheduler_type == 'per_batch':\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "    def post_val_batch(self):\n",
    "        pass\n",
    "\n",
    "    def post_train_stage(self):\n",
    "        pass\n",
    "\n",
    "    def post_val_stage(self, val_loss):\n",
    "        # called after every end of val stage (equals to epoch end)\n",
    "        if self.lr_scheduler is not None and self.lr_scheduler_type == 'per_epoch':\n",
    "            self.lr_scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 optimizer,\n",
    "                 train_dataset: Dataset,\n",
    "                 val_dataset: Dataset,\n",
    "                 tboard_log_dir: str = './tboard_logs/',\n",
    "                 batch_size: int = 128):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.train_writer = SummaryWriter(log_dir=tboard_log_dir + \"train/\")\n",
    "        self.val_writer = SummaryWriter(log_dir=tboard_log_dir + \"val/\")\n",
    "        self.cache = self.cache_states()\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def train(self, num_epochs: int):\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=True, pin_memory=True, batch_size=self.batch_size)\n",
    "        val_loader = DataLoader(self.val_dataset, shuffle=False, pin_memory=True, batch_size=self.batch_size)\n",
    "        best_loss = float('inf') # +inf\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch in tqdm(train_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                all_grads = []\n",
    "                for name, p in model.named_parameters():\n",
    "                    v = np.linalg.norm(p.grad.data.cpu().numpy())\n",
    "                    all_grads += [v]\n",
    "                    if \"weight\" in name:\n",
    "                        self.train_writer.add_scalar(f\"grad_{name}\", v, global_step=self.global_step)\n",
    "                        \n",
    "                self.train_writer.add_scalar(f\"grads\", np.mean(all_grads), global_step=self.global_step)\n",
    "                model.post_train_batch()\n",
    "                for k, v in details.items():\n",
    "                    self.train_writer.add_scalar(k, v, global_step=self.global_step)\n",
    "                self.global_step += 1\n",
    "\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            \n",
    "            val_logs = defaultdict(list)\n",
    "            for batch in tqdm(val_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "                val_losses.append(loss.item())\n",
    "                for k, v in details.items():\n",
    "                    val_logs[k].append(v)\n",
    "                    \n",
    "            val_logs = {k: np.mean(v) for k, v in val_logs.items()}\n",
    "            for k, v in val_logs.items():\n",
    "                self.val_writer.add_scalar(k, v, global_step=self.global_step)\n",
    "\n",
    "            val_loss = np.mean(val_losses)\n",
    "            model.post_val_stage(val_loss)\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                self.save_checkpoint(\"./best_checkpoint.pth\")\n",
    "                best_loss = val_loss\n",
    "\n",
    "    def find_lr(self, min_lr: float = 1e-6,\n",
    "                max_lr: float = 1e-1,\n",
    "                num_lrs: int = 20,\n",
    "                smooth_beta: float = 0.8) -> dict:\n",
    "        # exp((1-t) * log(left) + t * log(right))\n",
    "        lrs = np.geomspace(start=min_lr, stop=max_lr, num=num_lrs)\n",
    "        logs = {'lr': [], 'loss': [], 'avg_loss': []}\n",
    "        avg_loss = None\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\n",
    "\n",
    "        model.train()\n",
    "        for lr, batch in tqdm(zip(lrs, train_loader), desc='finding LR', total=num_lrs):\n",
    "            # apply new lr\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "            # train step\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            loss, details = model.compute_all(batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate smoothed loss\n",
    "            if avg_loss is None:\n",
    "                avg_loss = loss\n",
    "            else:\n",
    "                avg_loss = smooth_beta * avg_loss + (1 - smooth_beta) * loss\n",
    "\n",
    "            # store values into logs\n",
    "            logs['lr'].append(lr)\n",
    "            logs['avg_loss'].append(avg_loss)\n",
    "            logs['loss'].append(loss)\n",
    "\n",
    "        logs.update({key: np.array(val) for key, val in logs.items()})\n",
    "        self.rollback_states()\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def cache_states(self):\n",
    "        cache_dict = {'model_state': deepcopy(self.model.state_dict()),\n",
    "                      'optimizer_state': deepcopy(self.optimizer.state_dict())}\n",
    "\n",
    "        return cache_dict\n",
    "\n",
    "    def rollback_states(self):\n",
    "        self.model.load_state_dict(self.cache['model_state'])\n",
    "        self.optimizer.load_state_dict(self.cache['optimizer_state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]/home/m12sl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 469/469 [00:12<00:00, 36.48it/s]\n",
      "100%|██████████| 79/79 [00:01<00:00, 73.28it/s]\n",
      "100%|██████████| 469/469 [00:15<00:00, 30.55it/s]\n",
      "100%|██████████| 79/79 [00:01<00:00, 56.67it/s]\n",
      "100%|██████████| 469/469 [00:12<00:00, 38.71it/s]\n",
      "100%|██████████| 79/79 [00:01<00:00, 71.22it/s]\n",
      "100%|██████████| 469/469 [00:14<00:00, 32.52it/s]\n",
      "100%|██████████| 79/79 [00:01<00:00, 47.53it/s]\n",
      "100%|██████████| 469/469 [00:09<00:00, 48.16it/s]\n",
      "100%|██████████| 79/79 [00:00<00:00, 85.24it/s]\n"
     ]
    }
   ],
   "source": [
    "model = VeryModel()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "trainset = VectorSet(train=True)\n",
    "valset = VectorSet(train=False)\n",
    "\n",
    "trainer = Trainer(model, opt, trainset, valset, batch_size=128)\n",
    "trainer.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset) // 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "finding LR:   0%|          | 0/100 [00:00<?, ?it/s]/home/m12sl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "finding LR: 100%|██████████| 100/100 [00:02<00:00, 41.31it/s]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "logs = trainer.find_lr(min_lr=1e-4, max_lr=0.3, num_lrs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(logs['lr'], logs['loss'])\n",
    "plt.plot(logs['lr'], logs['avg_loss'])\n",
    "# plt.plot(logs['lr'])\n",
    "# plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7fd6dc338a60>, {0: 1, 1: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# x = dict()\n",
    "# x[0]\n",
    "\n",
    "\n",
    "x = defaultdict(lambda: 1)\n",
    "x[0]\n",
    "x[1]\n",
    "print(x)\n",
    "# def __getitem__(self, key):\n",
    "#     if key not in self.__dict__:\n",
    "#         self.__dict__[key] = self.init_fn()\n",
    "\n",
    "\n",
    "\n",
    "[{\"a\": 0}, {\"a\": 10}, {\"a\": 20}] -> {\"a\": [0, 10, 20, ...]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Хуки\n",
    "В случае, если нет возможности использовать интерактивную отладку или добавить print, очень удобным может оказаться добавление forward/backward хуков: функций, которые сработают при вызове forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "watches = {}\n",
    "def hook_fn(module, inp, out):\n",
    "    watches[module] = out.detach()\n",
    "\n",
    "for name, layer in nn._modules.items():\n",
    "    layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet34\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nn = resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m<ipython-input-32-1b886f713a5c>\u001b[0m(3)\u001b[0;36mhook_fn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      2 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mhook_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 3 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m\u001b[0;31m#     watches[module] = out.detach()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> print(module)\n",
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "ipdb> print(inp)\n",
      "(tensor([[[[0.6405, 0.6522, 0.6602,  ..., 0.8592, 0.0025, 0.6803],\n",
      "          [0.8846, 0.1048, 0.6138,  ..., 0.5477, 0.9546, 0.5978],\n",
      "          [0.1977, 0.6017, 0.6407,  ..., 0.7644, 0.8013, 0.2072],\n",
      "          ...,\n",
      "          [0.8693, 0.8641, 0.8068,  ..., 0.0583, 0.4243, 0.3765],\n",
      "          [0.4424, 0.2710, 0.8992,  ..., 0.2351, 0.7283, 0.8143],\n",
      "          [0.6501, 0.3726, 0.7520,  ..., 0.4047, 0.1835, 0.5901]],\n",
      "\n",
      "         [[0.9451, 0.4550, 0.6606,  ..., 0.8228, 0.6265, 0.0463],\n",
      "          [0.5665, 0.3650, 0.5102,  ..., 0.9672, 0.5110, 0.1856],\n",
      "          [0.1578, 0.8844, 0.8374,  ..., 0.2971, 0.7097, 0.8809],\n",
      "          ...,\n",
      "          [0.8550, 0.8532, 0.5854,  ..., 0.6965, 0.8541, 0.9085],\n",
      "          [0.9440, 0.0189, 0.3011,  ..., 0.6670, 0.0453, 0.5502],\n",
      "          [0.5023, 0.3918, 0.5710,  ..., 0.5511, 0.2801, 0.5674]],\n",
      "\n",
      "         [[0.9378, 0.5533, 0.9289,  ..., 0.0964, 0.1264, 0.9188],\n",
      "          [0.7204, 0.5154, 0.5210,  ..., 0.3986, 0.2537, 0.5081],\n",
      "          [0.3915, 0.4354, 0.5757,  ..., 0.0185, 0.9858, 0.9630],\n",
      "          ...,\n",
      "          [0.4446, 0.2691, 0.1722,  ..., 0.4552, 0.6587, 0.2724],\n",
      "          [0.3679, 0.0207, 0.1305,  ..., 0.9628, 0.6976, 0.6238],\n",
      "          [0.0521, 0.9610, 0.0120,  ..., 0.8920, 0.4412, 0.7505]]],\n",
      "\n",
      "\n",
      "        [[[0.1417, 0.8288, 0.8374,  ..., 0.8484, 0.5431, 0.5229],\n",
      "          [0.2198, 0.0211, 0.5072,  ..., 0.5783, 0.8051, 0.5276],\n",
      "          [0.1996, 0.2243, 0.6728,  ..., 0.3451, 0.0609, 0.2413],\n",
      "          ...,\n",
      "          [0.7553, 0.8375, 0.1008,  ..., 0.2818, 0.8423, 0.2816],\n",
      "          [0.5971, 0.7314, 0.5282,  ..., 0.3963, 0.8852, 0.4905],\n",
      "          [0.4510, 0.0204, 0.2190,  ..., 0.1471, 0.4611, 0.4027]],\n",
      "\n",
      "         [[0.1781, 0.5469, 0.8332,  ..., 0.3351, 0.6969, 0.2455],\n",
      "          [0.5085, 0.8160, 0.0843,  ..., 0.0370, 0.2922, 0.6340],\n",
      "          [0.7600, 0.6873, 0.6416,  ..., 0.3342, 0.4604, 0.8886],\n",
      "          ...,\n",
      "          [0.4290, 0.1609, 0.7109,  ..., 0.7329, 0.9919, 0.1448],\n",
      "          [0.1713, 0.9543, 0.2414,  ..., 0.9922, 0.3648, 0.6652],\n",
      "          [0.5892, 0.9963, 0.1624,  ..., 0.0982, 0.2457, 0.9443]],\n",
      "\n",
      "         [[0.9932, 0.6337, 0.7194,  ..., 0.2063, 0.7644, 0.9693],\n",
      "          [0.4887, 0.5739, 0.2197,  ..., 0.1218, 0.7085, 0.0455],\n",
      "          [0.4681, 0.4461, 0.3463,  ..., 0.8742, 0.1857, 0.9948],\n",
      "          ...,\n",
      "          [0.3448, 0.3226, 0.5945,  ..., 0.0197, 0.8074, 0.1646],\n",
      "          [0.0584, 0.0566, 0.9354,  ..., 0.1575, 0.6039, 0.6289],\n",
      "          [0.0268, 0.3191, 0.5967,  ..., 0.1184, 0.0322, 0.9522]]]]),)\n",
      "ipdb> print(out)\n",
      "tensor([[[[-3.7556e-01, -6.2316e-02, -2.2891e-01,  ..., -6.0655e-02,\n",
      "            1.3605e-01,  2.9846e-01],\n",
      "          [-8.3787e-01, -7.9136e-01, -9.2028e-01,  ..., -9.9815e-01,\n",
      "           -1.1550e+00, -4.8342e-01],\n",
      "          [ 9.6067e-02, -3.9465e-01, -4.1511e-01,  ..., -5.5884e-01,\n",
      "           -3.2537e-01, -1.0288e-01],\n",
      "          ...,\n",
      "          [-1.4711e-01, -2.2978e-01, -2.0403e-01,  ...,  6.2842e-02,\n",
      "           -5.8848e-03,  2.5054e-01],\n",
      "          [-5.9779e-02, -1.9278e-01, -1.9425e-01,  ..., -3.2177e-02,\n",
      "           -1.6252e-01,  9.4077e-02],\n",
      "          [-1.6244e+00, -2.3320e+00, -2.0167e+00,  ..., -2.4780e+00,\n",
      "           -2.4846e+00, -1.4610e+00]],\n",
      "\n",
      "         [[ 6.5993e-01, -2.7491e-01,  1.0830e-01,  ..., -7.3177e-01,\n",
      "           -5.3207e-01, -6.1838e-01],\n",
      "          [-3.5522e-01, -1.0528e-01, -4.0455e-01,  ..., -7.1035e-01,\n",
      "            4.8696e-02, -1.1845e+00],\n",
      "          [-6.0602e-01, -1.4066e-02,  7.5028e-03,  ..., -1.1764e-01,\n",
      "            7.0606e-01, -1.9238e+00],\n",
      "          ...,\n",
      "          [-3.0273e-01, -7.3018e-01,  2.4847e-01,  ...,  4.8674e-01,\n",
      "           -2.2057e-01, -4.1804e-01],\n",
      "          [ 1.0887e-02, -7.1287e-02,  2.9003e-02,  ...,  5.6539e-01,\n",
      "            2.1142e-01, -1.6881e+00],\n",
      "          [ 5.8689e-01, -1.6401e-01,  2.7276e-01,  ...,  1.3870e-01,\n",
      "            1.3422e-02, -2.0851e+00]],\n",
      "\n",
      "         [[-7.0566e-02, -4.6214e-01,  1.0272e-01,  ..., -1.5264e-01,\n",
      "           -9.3853e-01,  1.4132e-02],\n",
      "          [ 5.7552e-01, -1.6239e-01,  2.2187e-01,  ...,  5.5629e-02,\n",
      "            5.6102e-01, -4.0393e-01],\n",
      "          [-7.3072e-01, -3.9225e-01,  9.3594e-01,  ..., -3.7684e-01,\n",
      "           -6.5858e-01,  1.3895e+00],\n",
      "          ...,\n",
      "          [ 1.3324e-01,  4.3563e-01,  2.2125e-01,  ...,  3.1713e-01,\n",
      "            3.9914e-01, -5.2414e-01],\n",
      "          [-6.0933e-01, -4.5973e-01,  7.9502e-01,  ..., -8.2170e-01,\n",
      "           -6.3328e-01,  7.1777e-01],\n",
      "          [-1.8144e-01,  2.9690e-02, -3.1328e-01,  ...,  4.1236e-02,\n",
      "           -3.9398e-01, -2.5630e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.2741e-01, -1.8238e-03, -7.5584e-02,  ...,  3.9888e-01,\n",
      "           -3.9740e-01,  1.2892e+00],\n",
      "          [-1.0066e+00,  1.5272e-01,  4.2079e-01,  ..., -1.1183e+00,\n",
      "            5.6722e-01,  6.6497e-02],\n",
      "          [ 7.9518e-02, -4.4654e-01,  2.6580e-01,  ...,  8.2171e-02,\n",
      "            6.9800e-02, -1.8030e-01],\n",
      "          ...,\n",
      "          [-3.5340e-01, -1.1898e-01,  2.7139e-01,  ..., -4.9353e-01,\n",
      "           -1.6293e-01,  9.3682e-01],\n",
      "          [ 1.0648e-01,  4.8932e-01, -1.4110e-02,  ...,  3.0117e-01,\n",
      "            2.7818e-01, -4.1955e-01],\n",
      "          [-3.7645e-01, -6.9158e-01,  3.3641e-01,  ...,  1.4392e-01,\n",
      "            1.9987e-01, -9.7859e-01]],\n",
      "\n",
      "         [[ 4.8290e-01,  9.0624e-01,  1.0821e+00,  ...,  1.0582e+00,\n",
      "            9.8233e-01,  6.2684e-01],\n",
      "          [ 8.7248e-01,  1.3432e+00,  1.3309e+00,  ...,  1.5812e+00,\n",
      "            1.5380e+00,  1.0206e+00],\n",
      "          [ 3.0291e-01,  7.0329e-01,  7.8146e-01,  ...,  1.0429e+00,\n",
      "            8.2358e-01,  4.9484e-01],\n",
      "          ...,\n",
      "          [ 3.9412e-01,  7.8355e-01,  6.1965e-01,  ...,  5.3612e-01,\n",
      "            5.0060e-01,  1.0754e-01],\n",
      "          [ 4.8063e-01,  8.1406e-01,  6.2514e-01,  ...,  7.8018e-01,\n",
      "            1.0122e+00,  4.5620e-01],\n",
      "          [ 9.6125e-01,  1.3767e+00,  1.2912e+00,  ...,  1.5709e+00,\n",
      "            1.5939e+00,  1.0299e+00]],\n",
      "\n",
      "         [[ 1.2438e-07,  1.5811e-07,  2.3102e-07,  ...,  2.3692e-07,\n",
      "            2.3202e-07,  1.6606e-07],\n",
      "          [ 1.7949e-07,  2.4577e-07,  3.3908e-07,  ...,  3.5527e-07,\n",
      "            3.5454e-07,  2.5049e-07],\n",
      "          [ 1.8259e-07,  2.5195e-07,  3.2259e-07,  ...,  3.4739e-07,\n",
      "            3.6106e-07,  2.1707e-07],\n",
      "          ...,\n",
      "          [ 1.9504e-07,  2.6190e-07,  2.8715e-07,  ...,  3.7081e-07,\n",
      "            3.4601e-07,  2.0189e-07],\n",
      "          [ 1.9711e-07,  2.5606e-07,  3.1817e-07,  ...,  3.3083e-07,\n",
      "            3.2966e-07,  1.9978e-07],\n",
      "          [ 1.2816e-07,  1.6840e-07,  1.8538e-07,  ...,  2.1338e-07,\n",
      "            2.1148e-07,  1.0940e-07]]],\n",
      "\n",
      "\n",
      "        [[[-2.6536e-01,  1.0572e-01,  4.3759e-01,  ...,  1.9924e-01,\n",
      "           -5.8907e-02, -9.4530e-02],\n",
      "          [-3.6284e-01, -7.8521e-01, -1.2778e+00,  ..., -4.8495e-01,\n",
      "           -3.0342e-01,  3.3212e-03],\n",
      "          [-3.0837e-01, -4.4559e-01, -3.1256e-01,  ..., -7.9285e-01,\n",
      "           -1.0591e+00, -4.7286e-01],\n",
      "          ...,\n",
      "          [-4.3740e-01, -7.5017e-01, -7.8454e-01,  ..., -1.3692e+00,\n",
      "           -1.3350e+00, -6.2077e-01],\n",
      "          [-2.9727e-01, -6.1107e-01, -5.4614e-01,  ..., -4.3962e-01,\n",
      "           -1.5148e-01,  2.0017e-01],\n",
      "          [-1.4294e+00, -1.5527e+00, -1.6752e+00,  ..., -1.9722e+00,\n",
      "           -2.1471e+00, -1.3282e+00]],\n",
      "\n",
      "         [[-4.1203e-01,  1.5581e-01,  2.2309e-01,  ..., -7.0225e-01,\n",
      "            1.1344e+00, -1.0591e+00],\n",
      "          [-2.0970e-01,  6.5581e-01,  9.4265e-01,  ..., -7.1033e-01,\n",
      "            7.5291e-01, -2.0886e+00],\n",
      "          [-1.0118e-01,  5.9665e-01,  6.4366e-01,  ..., -7.3879e-01,\n",
      "            4.5183e-01, -1.6958e+00],\n",
      "          ...,\n",
      "          [ 4.2022e-01, -4.7354e-02, -3.5457e-01,  ...,  8.6639e-02,\n",
      "           -7.6705e-01, -1.6672e+00],\n",
      "          [-7.5846e-02, -9.7000e-02, -1.3358e-01,  ...,  2.5689e-01,\n",
      "           -9.6398e-02, -1.2370e+00],\n",
      "          [-7.7765e-01, -2.6260e-01, -4.0183e-01,  ..., -6.2973e-02,\n",
      "            2.3013e-01, -9.5919e-01]],\n",
      "\n",
      "         [[ 5.6663e-01, -1.6210e+00,  3.6649e-01,  ...,  3.0547e-01,\n",
      "            4.6247e-02, -9.1396e-01],\n",
      "          [-2.8459e-01, -7.6657e-02,  9.8248e-01,  ..., -3.4212e-01,\n",
      "           -3.3102e-01,  7.5673e-01],\n",
      "          [-1.4955e-01, -2.5616e-02, -5.0178e-01,  ...,  1.9875e-01,\n",
      "            7.0546e-01, -8.7592e-01],\n",
      "          ...,\n",
      "          [ 4.3136e-01, -1.7159e+00,  5.3186e-01,  ...,  1.4453e-01,\n",
      "            1.0874e-01, -5.0332e-01],\n",
      "          [ 3.1925e-01,  6.2847e-02,  1.9775e-01,  ..., -4.6039e-01,\n",
      "           -1.4061e-02,  7.2623e-01],\n",
      "          [-1.1032e-01, -3.0440e-01, -2.0860e-01,  ...,  5.3297e-02,\n",
      "            7.2454e-01, -1.3138e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2738e-01, -6.0329e-01,  1.1899e+00,  ..., -9.9874e-01,\n",
      "            1.0962e+00,  4.2795e-01],\n",
      "          [-5.1837e-01,  7.7895e-01, -1.9999e-01,  ...,  1.1565e+00,\n",
      "            1.2146e-01, -9.6472e-01],\n",
      "          [-1.7728e-01,  1.5298e-01,  4.5715e-01,  ..., -8.7124e-01,\n",
      "           -4.4022e-01,  7.2652e-01],\n",
      "          ...,\n",
      "          [ 3.2351e-01, -5.6546e-01,  3.7456e-01,  ...,  8.5553e-01,\n",
      "           -5.9629e-02, -1.3249e+00],\n",
      "          [-6.2946e-01,  5.4073e-01, -2.7103e-01,  ..., -6.3360e-01,\n",
      "           -3.8106e-01,  1.5858e+00],\n",
      "          [-9.2146e-01,  3.2267e-01, -4.5451e-01,  ..., -2.7244e-01,\n",
      "            4.7431e-01, -1.0104e+00]],\n",
      "\n",
      "         [[ 4.6497e-01,  7.5223e-01,  8.4003e-01,  ...,  8.5043e-01,\n",
      "            9.0418e-01,  7.9619e-01],\n",
      "          [ 7.0125e-01,  1.1058e+00,  1.4548e+00,  ...,  1.3548e+00,\n",
      "            1.2606e+00,  8.2054e-01],\n",
      "          [ 5.0476e-01,  8.2278e-01,  9.8578e-01,  ...,  8.0004e-01,\n",
      "            1.2581e+00,  7.0360e-01],\n",
      "          ...,\n",
      "          [ 5.8289e-01,  1.1825e+00,  1.0371e+00,  ...,  1.2159e+00,\n",
      "            1.4567e+00,  7.6556e-01],\n",
      "          [ 5.7121e-01,  8.4609e-01,  7.5793e-01,  ...,  9.0281e-01,\n",
      "            6.8897e-01,  2.6306e-01],\n",
      "          [ 7.5895e-01,  1.0487e+00,  8.9945e-01,  ...,  1.2424e+00,\n",
      "            1.2932e+00,  9.6492e-01]],\n",
      "\n",
      "         [[ 1.1951e-07,  1.6640e-07,  2.0863e-07,  ...,  2.2875e-07,\n",
      "            1.9665e-07,  1.4649e-07],\n",
      "          [ 1.8793e-07,  2.6746e-07,  3.2488e-07,  ...,  3.4201e-07,\n",
      "            3.3194e-07,  2.4651e-07],\n",
      "          [ 1.8318e-07,  2.7551e-07,  3.6042e-07,  ...,  3.7071e-07,\n",
      "            3.5217e-07,  2.2027e-07],\n",
      "          ...,\n",
      "          [ 1.8894e-07,  2.3747e-07,  2.8666e-07,  ...,  2.9975e-07,\n",
      "            2.9469e-07,  2.0231e-07],\n",
      "          [ 1.9071e-07,  2.4597e-07,  2.9648e-07,  ...,  3.0882e-07,\n",
      "            3.1888e-07,  1.8685e-07],\n",
      "          [ 1.1917e-07,  1.5296e-07,  1.6025e-07,  ...,  1.7782e-07,\n",
      "            1.8093e-07,  9.4733e-08]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> clear\n",
      "Clear all breaks? clrscr\n",
      "ipdb> clr\n",
      "*** NameError: name 'clr' is not defined\n",
      "ipdb> clear\n",
      "Clear all breaks? y\n",
      "ipdb> c\n",
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m<ipython-input-32-1b886f713a5c>\u001b[0m(3)\u001b[0;36mhook_fn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      2 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mhook_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 3 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m\u001b[0;31m#     watches[module] = out.detach()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> module\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ipdb> exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9757b793f045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-1b886f713a5c>\u001b[0m in \u001b[0;36mhook_fn\u001b[0;34m(module, inp, out)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhook_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     watches[module] = out.detach()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'return'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exception'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_return\u001b[0;34m(self, frame, arg)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_returning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;31m# The user issued a 'next' or 'until' command.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoplineno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 224, 224)\n",
    "out = nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight 11.935171 1.0416042e-05\n",
      "bn1.weight 2.3838437 2.4423252e-06\n",
      "bn1.bias 2.7731774 5.316256e-07\n",
      "layer1.0.conv1.weight 8.189711 6.8766776e-06\n",
      "layer1.0.bn1.weight 2.3587146 1.140095e-06\n",
      "layer1.0.bn1.bias 1.378745 9.216205e-07\n",
      "layer1.0.conv2.weight 6.6911764 4.712874e-06\n",
      "layer1.0.bn2.weight 2.1586852 1.2505008e-06\n",
      "layer1.0.bn2.bias 1.438372 4.544441e-07\n",
      "layer1.1.conv1.weight 7.1646214 3.59087e-06\n",
      "layer1.1.bn1.weight 2.1580186 8.555862e-07\n",
      "layer1.1.bn1.bias 1.2942387 6.894537e-07\n",
      "layer1.1.conv2.weight 6.500586 3.821854e-06\n",
      "layer1.1.bn2.weight 2.2491286 8.6917845e-07\n",
      "layer1.1.bn2.bias 1.0933689 3.6934185e-07\n",
      "layer1.2.conv1.weight 6.9216113 3.819248e-06\n",
      "layer1.2.bn1.weight 2.1611712 1.0814472e-06\n",
      "layer1.2.bn1.bias 1.344261 7.086904e-07\n",
      "layer1.2.conv2.weight 6.1577663 3.758724e-06\n",
      "layer1.2.bn2.weight 2.2449765 7.495827e-07\n",
      "layer1.2.bn2.bias 0.85555875 3.0305264e-07\n",
      "layer2.0.conv1.weight 9.547822 6.9345383e-06\n",
      "layer2.0.bn1.weight 3.0421858 1.6717302e-06\n",
      "layer2.0.bn1.bias 1.1591606 1.2672202e-06\n",
      "layer2.0.conv2.weight 10.538155 8.8326515e-06\n",
      "layer2.0.bn2.weight 3.2247 1.3181029e-06\n",
      "layer2.0.bn2.bias 1.2064401 9.586537e-07\n",
      "layer2.0.downsample.0.weight 6.3643017 3.621425e-06\n",
      "layer2.0.downsample.1.weight 2.864167 1.1762887e-06\n",
      "layer2.0.downsample.1.bias 1.2064401 9.586537e-07\n",
      "layer2.1.conv1.weight 9.492722 7.0064816e-06\n",
      "layer2.1.bn1.weight 2.741177 1.3856167e-06\n",
      "layer2.1.bn1.bias 1.829177 1.0246858e-06\n",
      "layer2.1.conv2.weight 8.996571 6.3061802e-06\n",
      "layer2.1.bn2.weight 2.3102362 1.2659259e-06\n",
      "layer2.1.bn2.bias 1.536188 8.451506e-07\n",
      "layer2.2.conv1.weight 9.953475 7.2130383e-06\n",
      "layer2.2.bn1.weight 2.8997393 1.7288822e-06\n",
      "layer2.2.bn1.bias 2.3190634 1.1953277e-06\n",
      "layer2.2.conv2.weight 8.704957 6.2458753e-06\n",
      "layer2.2.bn2.weight 2.1118746 1.1929859e-06\n",
      "layer2.2.bn2.bias 1.3660517 7.196373e-07\n",
      "layer2.3.conv1.weight 10.341978 7.22507e-06\n",
      "layer2.3.bn1.weight 2.794896 1.5009342e-06\n",
      "layer2.3.bn1.bias 2.8062103 9.24931e-07\n",
      "layer2.3.conv2.weight 8.0158205 5.54043e-06\n",
      "layer2.3.bn2.weight 2.2654805 1.1839657e-06\n",
      "layer2.3.bn2.bias 1.5399693 5.3739075e-07\n",
      "layer3.0.conv1.weight 13.566901 8.3941e-06\n",
      "layer3.0.bn1.weight 4.4006395 1.3760657e-06\n",
      "layer3.0.bn1.bias 1.761137 1.0596262e-06\n",
      "layer3.0.conv2.weight 16.468916 8.433869e-06\n",
      "layer3.0.bn2.weight 4.664263 1.318387e-06\n",
      "layer3.0.bn2.bias 1.1461083 9.667388e-07\n",
      "layer3.0.downsample.0.weight 7.043852 2.9541127e-06\n",
      "layer3.0.downsample.1.weight 2.308922 9.515013e-07\n",
      "layer3.0.downsample.1.bias 1.1461083 9.667388e-07\n",
      "layer3.1.conv1.weight 12.998692 7.4815357e-06\n",
      "layer3.1.bn1.weight 3.7592928 1.6660275e-06\n",
      "layer3.1.bn1.bias 2.941394 1.1522442e-06\n",
      "layer3.1.conv2.weight 12.502084 6.533328e-06\n",
      "layer3.1.bn2.weight 2.7279325 1.3002879e-06\n",
      "layer3.1.bn2.bias 1.7639009 9.0652304e-07\n",
      "layer3.2.conv1.weight 12.80605 7.81879e-06\n",
      "layer3.2.bn1.weight 3.5300686 1.7657088e-06\n",
      "layer3.2.bn1.bias 3.690631 1.1771648e-06\n",
      "layer3.2.conv2.weight 11.906655 6.3360285e-06\n",
      "layer3.2.bn2.weight 2.8409345 1.0184375e-06\n",
      "layer3.2.bn2.bias 1.4911708 7.4869166e-07\n",
      "layer3.3.conv1.weight 12.404214 7.39299e-06\n",
      "layer3.3.bn1.weight 3.3700585 1.8267613e-06\n",
      "layer3.3.bn1.bias 3.8534412 1.0656286e-06\n",
      "layer3.3.conv2.weight 11.167714 5.5636556e-06\n",
      "layer3.3.bn2.weight 2.9882567 1.01387e-06\n",
      "layer3.3.bn2.bias 1.9576664 6.654279e-07\n",
      "layer3.4.conv1.weight 12.724505 6.469272e-06\n",
      "layer3.4.bn1.weight 3.4326274 1.7788035e-06\n",
      "layer3.4.bn1.bias 3.982021 1.1015323e-06\n",
      "layer3.4.conv2.weight 11.238367 5.1574298e-06\n",
      "layer3.4.bn2.weight 3.136275 9.859687e-07\n",
      "layer3.4.bn2.bias 2.286069 6.06983e-07\n",
      "layer3.5.conv1.weight 13.007959 5.807649e-06\n",
      "layer3.5.bn1.weight 3.6052737 1.3292686e-06\n",
      "layer3.5.bn1.bias 3.7872233 7.672349e-07\n",
      "layer3.5.conv2.weight 11.49065 4.9857294e-06\n",
      "layer3.5.bn2.weight 3.2391326 8.024444e-07\n",
      "layer3.5.bn2.bias 3.2612906 5.1534397e-07\n",
      "layer4.0.conv1.weight 19.833477 6.370545e-06\n",
      "layer4.0.bn1.weight 5.789732 1.1697173e-06\n",
      "layer4.0.bn1.bias 4.5516257 7.6250967e-07\n",
      "layer4.0.conv2.weight 21.406986 7.1847476e-06\n",
      "layer4.0.bn2.weight 16.778883 4.2757773e-07\n",
      "layer4.0.bn2.bias 2.4519036 4.1399431e-07\n",
      "layer4.0.downsample.0.weight 9.536392 2.6255211e-06\n",
      "layer4.0.downsample.1.weight 10.09241 2.7321548e-07\n",
      "layer4.0.downsample.1.bias 2.4519036 4.1399431e-07\n",
      "layer4.1.conv1.weight 20.015196 6.473762e-06\n",
      "layer4.1.bn1.weight 5.3224654 1.4267987e-06\n",
      "layer4.1.bn1.bias 4.9684362 1.1611626e-06\n",
      "layer4.1.conv2.weight 17.692015 5.2899163e-06\n",
      "layer4.1.bn2.weight 13.999152 4.152235e-07\n",
      "layer4.1.bn2.bias 3.1741066 5.467223e-07\n",
      "layer4.2.conv1.weight 21.19884 4.152692e-06\n",
      "layer4.2.bn1.weight 5.559282 8.937586e-07\n",
      "layer4.2.bn1.bias 4.627881 6.514034e-07\n",
      "layer4.2.conv2.weight 16.622658 4.6555197e-06\n",
      "layer4.2.bn2.weight 31.398672 4.342724e-07\n",
      "layer4.2.bn2.bias 3.24832 7.3570425e-07\n",
      "fc.weight 45.79176 0.67184997\n",
      "fc.bias 0.37206826 0.03162278\n"
     ]
    }
   ],
   "source": [
    "all_grads = []\n",
    "\n",
    "for name, param in nn.named_parameters():\n",
    "    x = param.data.cpu().numpy().reshape(-1)\n",
    "    x = np.sqrt((x ** 2).sum())\n",
    "    t = param.grad.data.cpu().numpy().reshape(-1)\n",
    "    all_grads.append(t)\n",
    "    t = np.sqrt((t ** 2).sum())\n",
    "    print(name, x, t)\n",
    "\n",
    "\n",
    "x = np.concatenate(all_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67259514"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
