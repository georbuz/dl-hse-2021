{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIu1b5Xo4Bdd"
   },
   "source": [
    "# NN debug\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m12sl/dl-hse-2021/blob/master/04-debug/homework.ipynb)\n",
    "\n",
    "В этой тетрадке мы рассмотрим несколько проблем с обучением сеток и способы их решения.\n",
    "\n",
    "*Лучше решать эту домашку в колабе*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyTa9uFb4Bdl"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiLnuFXG4Bdm"
   },
   "source": [
    "# Data\n",
    "\n",
    "Для обучения сеток мы будем использовать MNIST.\n",
    "\n",
    "Качаем архив [Google Drive](https://drive.google.com/file/d/1xo-AIG2E6cTZbWGti1A5lp5FDtf4aHx_/view?usp=sharing). \n",
    "Его структура следующая:\n",
    "- /\n",
    "    - /train.csv\n",
    "    - /val.csv\n",
    "    - /train/{image_name}.png\n",
    "    - /val/{image_name}.png\n",
    "\n",
    "CSV файлы содержат название файла и его лейбл: image_name, label.\n",
    "\n",
    "Распакуйте архив в текущую папку:\n",
    "`unzip -q ./mnist_data2.zip -d ./`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmvTQPTN4Bdo"
   },
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, images_dir_path: str,\n",
    "                 description_csv_path: str):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.images_dir_path = images_dir_path\n",
    "        self.description_df = pd.read_csv(description_csv_path,\n",
    "                                           dtype={'image_name': str, 'label': int})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.description_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name, label = self.description_df.iloc[index, :]\n",
    "        \n",
    "        img_path = Path(self.images_dir_path, f'{img_name}.png')\n",
    "        img = self._read_img(img_path)\n",
    "        \n",
    "        return dict(sample=img, label=label)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _read_img(img_path: Path):\n",
    "        img = cv2.imread(str(img_path.resolve()))\n",
    "        img = img.astype(np.float32)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8b_O3Y94Bdq"
   },
   "source": [
    "## Задание 1\n",
    "**(0.4 балла)** Запустите обучение сети в ячейках ниже. За 10 эпох метрика на валидации вырастает всего до ~0.15.\n",
    "\n",
    "*Вопросы:*\n",
    "1. Почему сетка так плохо учится?\n",
    "1. Найдите ошибку в коде и объясните ошибка вызывает подобное поведение в обучении?\n",
    "\n",
    "*Requirements:*\n",
    "1. Напишите ответы в markdown ячейке перед следующим заданием\n",
    "1. В следующей ячейке (после вашего ответа) вставьте код с исправлением ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6TO3Gc1rD6t"
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = resnet18()\n",
    "        self.net.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def compute_all(self, batch):\n",
    "        x = batch['sample'] / 255.0\n",
    "        y = batch['label']\n",
    "        logits = self.net(x)\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = (logits.argmax(axis=1) == y).float().mean().cpu().numpy()\n",
    "        metrics = dict(acc=acc)\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 optimizer,\n",
    "                 train_dataset: Dataset,\n",
    "                 val_dataset: Dataset,\n",
    "                 tboard_log_dir: str,\n",
    "                 batch_size: int = 128):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.log_writer = SummaryWriter(log_dir=tboard_log_dir)\n",
    "\n",
    "    def train(self, num_epochs: int):\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=False, batch_size=self.batch_size)\n",
    "        val_loader = DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size)\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch in tqdm(train_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                for k, v in details.items():\n",
    "                    self.log_writer.add_scalar(k, v, global_step=self.global_step)\n",
    "                self.global_step += 1\n",
    "\n",
    "            model.eval()\n",
    "            val_losses, val_metrics_list = [], []\n",
    "            for batch in tqdm(val_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "                val_losses.append(loss.item())\n",
    "                val_metrics_list.append(details['acc'].item())\n",
    "\n",
    "            val_loss, val_metrics = np.mean(val_losses), np.mean(val_metrics_list)\n",
    "            self.log_writer.add_scalar('val/loss', val_loss, global_step=self.global_step)\n",
    "            self.log_writer.add_scalar('val/metrics', val_metrics, global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJ0jh7Te4Bdq"
   },
   "outputs": [],
   "source": [
    "mnist_train = MNISTDataset(images_dir_path='./mnist_data/train/',\n",
    "                           description_csv_path='./mnist_data/train.csv')\n",
    "mnist_val = MNISTDataset(images_dir_path='./mnist_data/val/',\n",
    "                         description_csv_path='./mnist_data/val.csv')\n",
    "\n",
    "model = ResNet18()\n",
    "opt = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "trainer = Trainer(model=model, optimizer=opt, train_dataset=mnist_train,\n",
    "                  val_dataset=mnist_val, tboard_log_dir='./tboard_logs/exp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxYrh8Ky4Bdr"
   },
   "outputs": [],
   "source": [
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LhP5QqMAQxz"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lsEvSVk_p72"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./tboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pxac_l_74Bds"
   },
   "source": [
    "## Задание 2\n",
    "**(0.2 балла)** Запустите обучение сети в ячейках ниже. За 10 эпох сетка не покажет качества выше случайного угадывания.\n",
    "\n",
    "*Вопросы:*\n",
    "1. Почему сетка так плохо учится?\n",
    "1. Найдите ошибку в коде и объясните почему найденная ошибка вызывает подобное поведение в обучении?\n",
    "\n",
    "*Requirements:*\n",
    "1. Напишите ответы в markdown ячейке перед следующим заданием\n",
    "1. В следующей ячейке (после вашего ответа) вставьте код с исправлением ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfyrLn8qrMOS"
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = resnet18()\n",
    "        self.net.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def compute_all(self, batch):\n",
    "        x = batch['sample'] / 255.0\n",
    "        y = batch['label']\n",
    "        logits = self.net(x)\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = (logits.argmax(axis=1) == y).float().mean().cpu().numpy()\n",
    "        metrics = dict(acc=acc)\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 optimizer,\n",
    "                 train_dataset: Dataset,\n",
    "                 val_dataset: Dataset,\n",
    "                 tboard_log_dir: str,\n",
    "                 batch_size: int = 128):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.log_writer = SummaryWriter(log_dir=tboard_log_dir)\n",
    "\n",
    "    def train(self, num_epochs: int):\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\n",
    "        val_loader = DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size)\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch in tqdm(train_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                for k, v in details.items():\n",
    "                    self.log_writer.add_scalar(k, v, global_step=self.global_step)\n",
    "                self.global_step += 1\n",
    "\n",
    "            model.eval()\n",
    "            val_losses, val_metrics_list = [], []\n",
    "            for batch in tqdm(val_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "                val_losses.append(loss.item())\n",
    "                val_metrics_list.append(details['acc'].item())\n",
    "\n",
    "            val_loss, val_metrics = np.mean(val_losses), np.mean(val_metrics_list)\n",
    "            self.log_writer.add_scalar('val/loss', val_loss, global_step=self.global_step)\n",
    "            self.log_writer.add_scalar('val/metrics', val_metrics, global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTsIwDwF4Bds"
   },
   "outputs": [],
   "source": [
    "mnist_train = MNISTDataset(images_dir_path='./mnist_data/train/',\n",
    "                           description_csv_path='./mnist_data/train.csv')\n",
    "mnist_val = MNISTDataset(images_dir_path='./mnist_data/val/',\n",
    "                         description_csv_path='./mnist_data/val.csv')\n",
    "\n",
    "model = ResNet18()\n",
    "opt = optim.SGD(model.parameters(), lr=10e-2, weight_decay=9e-1)\n",
    "\n",
    "trainer = Trainer(model=model, optimizer=opt, train_dataset=mnist_train,\n",
    "                  val_dataset=mnist_val, tboard_log_dir='./tboard_logs/exp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvpEK2ornFb_"
   },
   "outputs": [],
   "source": [
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAWnZdpcp7hE"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./tboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jn7r5QBD4Bds"
   },
   "source": [
    "## Задание 3\n",
    "**(0.4 балла)** Запустите обучение сети в ячейках ниже. В сети будут использоваться предобученные параметры, которые должны были помочь выдавать качество около 1. Однако, за 5 эпох сетка не выдаст качество, которое мы ожидали.\n",
    "\n",
    "Перед запуском ячеек скачайте используемое состояние модели [pretrained_model.pt](https://drive.google.com/file/d/1JITAz1L8mWpTGany84YMYKIhzVgsBf_9/view?usp=sharing).\n",
    "\n",
    "*Вопросы:*\n",
    "1. Почему сетка так плохо учится?\n",
    "1. Найдите ошибку и объясните почему найденная ошибка вызывает подобное поведение в обучении?\n",
    "\n",
    "*Requirements:*\n",
    "1. Напишите ответы в markdown ячейке после ячейки с тензорбордом.\n",
    "1. В следующей ячейке (после вашего ответа) вставьте код с исправлением ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibwv4nXw4Bds"
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = resnet18()\n",
    "        self.net.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def compute_all(self, batch):\n",
    "        x = batch['sample'] / 255.0\n",
    "        y = batch['label']\n",
    "        logits = self.net(x)\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = (logits.argmax(axis=1) == y).float().mean().cpu().numpy()\n",
    "        metrics = dict(acc=acc)\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 optimizer,\n",
    "                 train_dataset: Dataset,\n",
    "                 val_dataset: Dataset,\n",
    "                 tboard_log_dir: str,\n",
    "                 batch_size: int = 128):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.log_writer = SummaryWriter(log_dir=tboard_log_dir)\n",
    "\n",
    "    def train(self, num_epochs: int):\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\n",
    "        val_loader = DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size)\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch in tqdm(train_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                for k, v in details.items():\n",
    "                    self.log_writer.add_scalar(k, v, global_step=self.global_step)\n",
    "                self.global_step += 1\n",
    "\n",
    "            model.eval()\n",
    "            val_losses, val_metrics_list = [], []\n",
    "            for batch in tqdm(val_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "                val_losses.append(loss.item())\n",
    "                val_metrics_list.append(details['acc'].item())\n",
    "\n",
    "            val_loss, val_metrics = np.mean(val_losses), np.mean(val_metrics_list)\n",
    "            self.log_writer.add_scalar('val/loss', val_loss, global_step=self.global_step)\n",
    "            self.log_writer.add_scalar('val/metrics', val_metrics, global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5kMzqkFK3qE"
   },
   "outputs": [],
   "source": [
    "mnist_train = MNISTDataset(images_dir_path='./mnist_data/train/',\n",
    "                           description_csv_path='./mnist_data/train.csv')\n",
    "mnist_val = MNISTDataset(images_dir_path='./mnist_data/val/',\n",
    "                         description_csv_path='./mnist_data/val.csv')\n",
    "\n",
    "model = ResNet18()\n",
    "model_sate_path = 'pretrained_model.pt'\n",
    "model.load_state_dict(torch.load(model_sate_path, map_location='cpu'))\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "trainer = Trainer(model=model, optimizer=opt, train_dataset=mnist_train,\n",
    "                  val_dataset=mnist_val, tboard_log_dir='./tboard_logs/exp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQBMJL3JK3v9"
   },
   "outputs": [],
   "source": [
    "trainer.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nX7EJujjLg_X"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./tboard_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6KxK4MzpCe9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "homework.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
