{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отладка моделей\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m12sl/dl-hse-2021/blob/master/04-debug/seminar.ipynb)\n",
    "\n",
    "\n",
    "План семинара:\n",
    "\n",
    "- [ ] Освоить LR scheduling\n",
    "- [ ] Написать LR range test\n",
    "- [ ] Разобраться с подсчетом валидационных и тренировочных метрик \n",
    "- [ ] Добавим логгирование норм градиентов\n",
    "- [ ] Посмотрим на forward-hook\n",
    "- [ ] Classier Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Scheduling\n",
    "\n",
    "Два типа расписаний:\n",
    "\n",
    "- по эпохам (StepLR, ReduceLROnPlateau, ...) \n",
    "    ```\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    for epoch in range(epochs):\n",
    "        train(...)\n",
    "        validate(...)\n",
    "        scheduler.step()\n",
    "    ```\n",
    "\n",
    "\n",
    "- по батчам (Cosine, Cyclic, 1cycle, ...)\n",
    "    ```\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
    "    for epoch in range(epochs):\n",
    "        # train(...)\n",
    "        for batch in data_loader:\n",
    "            train_batch(...)\n",
    "            scheduler.step()\n",
    "        # validate(...)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор оптимального LR\n",
    "\n",
    "\n",
    "Для выбора оптимального LR удобно использовать т.н. Learning Rate Range Test, часто процедуру называют просто find_lr. Под капотом проход по тренировочной эпохе с lr, изменяемым на каждом батче по формуле:\n",
    "\n",
    "$$\n",
    "\\mathrm{it} = \\frac{\\mathrm{step}}{\\mathrm{total steps}}\\\\\n",
    "\\mathrm{lr} = \\exp\\left\\{ \n",
    "    (1 - t ) \\log a + t \\log b\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "Чтобы поменять LR для всех оптимизируемых параметров, можно пройтись по ним циклом:\n",
    "\n",
    "```\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/02/lr_finder.png\"/>\n",
    "\n",
    "_картинка из бложика [Jeremy Jordan](https://www.jeremyjordan.me/nn-learning-rate/)_\n",
    "\n",
    "\n",
    "Идея приема простая: пока LR меньше некоторого порога на каждом шаге градиентного спуска веса просто не меняются (в частности из-за особенностей операций с плавающей точкой).\n",
    "При очень большом LR мы шагаем слишком далеко и уходим от точки экстремума. \n",
    "\n",
    "Оптимальный LR лежит где-то между ними. Экспоненциальная формула изменения LR позволяет с должным качеством найти хорошую точку.\n",
    "\n",
    "\n",
    "\n",
    "Если интересно: [статья , в которой эту технику предложили и активно использовали](https://arxiv.org/pdf/1506.01186.pdf).\n",
    "\n",
    "\n",
    "**Some math notes**\n",
    "\n",
    "У типов данных с плавающей точкой есть арифметические особенности:\n",
    "\n",
    "$$\n",
    "# fp32\n",
    "x + \\delta == x,\\,\\mathrm{если}\\; \\delta < 5.96 \\cdot 10^{-8} x\n",
    "$$\n",
    "\n",
    "К слову, это еще одна причина присматривать за величинами активаций, нормировать данные и таргет в случае регрессии. Можно было бы перейти на float64, но (вычислительно и по памяти) дешевле быть аккуратными на float32.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://blogs.nvidia.com/wp-content/uploads/2020/05/tf32-Mantissa-chart-hi-res-FINAL-400x255.png.webp\"/>\n",
    "\n",
    "_картинка из статьи [NVIDIA](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики\n",
    "\n",
    "TL; DR:\n",
    "- тренировочные метрики записывать без сглаживания с каждого батча\n",
    "- валидационные собирать за всю валидацию и рисовать одной точкой\n",
    "\n",
    "\n",
    "**Особенности TB**:\n",
    "\n",
    "- При отображении прореживает точки по global_step\n",
    "- Чтобы рисовать на одном графике надо писать в разные папки (завести отдельные train_ и val_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обновим Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeryModel(nn.Module):\n",
    "    def __init__(self, lr_scheduler=None, lr_scheduler_type=None):\n",
    "        super().__init__()\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.lr_scheduler_type = lr_scheduler_type\n",
    "        if lr_scheduler_type not in [None, 'per_batch', 'per_epoch']:\n",
    "            raise ValueError(\"lr_scheduler_type must be one of: None, 'per_batch', 'per_epoch'. \"\n",
    "                             f\"Not: {lr_scheduler_type}\")\n",
    "\n",
    "        self.inner = nn.Sequential(nn.Linear(784, 100),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(100, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inner(x)\n",
    "\n",
    "    def compute_all(self, batch):  # удобно сделать функцию, в которой вычисляется лосс по пришедшему батчу\n",
    "        x = batch['sample'] / 255.0\n",
    "        y = batch['label']\n",
    "        logits = self.inner(x)\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = (logits.argmax(axis=1) == y).float().mean().cpu().numpy()\n",
    "        metrics = dict(acc=acc)\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "    def post_train_batch(self):\n",
    "        # called after every train batch\n",
    "        if self.lr_scheduler is not None and self.lr_scheduler_type == 'per_batch':\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "    def post_val_batch(self):\n",
    "        pass\n",
    "\n",
    "    def post_train_stage(self):\n",
    "        pass\n",
    "\n",
    "    def post_val_stage(self, val_loss):\n",
    "        # called after every end of val stage (equals to epoch end)\n",
    "        if self.lr_scheduler is not None and self.lr_scheduler_type == 'per_epoch':\n",
    "            self.lr_scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 optimizer,\n",
    "                 train_dataset: Dataset,\n",
    "                 val_dataset: Dataset,\n",
    "                 tboard_log_dir: str = './tboard_logs/',\n",
    "                 batch_size: int = 128):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.log_writer = SummaryWriter(log_dir=tboard_log_dir)\n",
    "        self.cache = self.cache_states()\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def train(self, num_epochs: int):\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=True, pin_memory=True, batch_size=self.batch_size)\n",
    "        val_loader = DataLoader(self.val_dataset, shuffle=False, pin_memory=True, batch_size=self.batch_size)\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch in tqdm(train_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                #\n",
    "                for name, p in model.named_parameters():\n",
    "                    if \"weight\" in name:\n",
    "                        v = np.linalg.norm(p.grad.data.cpu().numpy())\n",
    "                        self.log_writer.add_scalar(f\"grad_{name}\", v, global_step=self.global_step)\n",
    "\n",
    "                model.post_train_batch()\n",
    "                for k, v in details.items():\n",
    "                    self.log_writer.add_scalar(k, v, global_step=self.global_step)\n",
    "                self.global_step += 1\n",
    "\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            for batch in tqdm(val_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "            val_loss = np.mean(val_losses)\n",
    "            model.post_val_stage(val_loss)\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                self.save_checkpoint(\"./best_checkpoint.pth\")\n",
    "                best_loss = val_loss\n",
    "\n",
    "    def find_lr(self, min_lr: float = 1e-6,\n",
    "                max_lr: float = 1e-1,\n",
    "                num_lrs: int = 20,\n",
    "                smooth_beta: float = 0.8) -> dict:\n",
    "        lrs = np.geomspace(start=min_lr, stop=max_lr, num=num_lrs)\n",
    "        logs = {'lr': [], 'loss': [], 'avg_loss': []}\n",
    "        avg_loss = None\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\n",
    "\n",
    "        model.train()\n",
    "        for lr, batch in tqdm(zip(lrs, train_loader), desc='finding LR', total=num_lrs):\n",
    "            # apply new lr\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "            # train step\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            loss, details = model.compute_all(batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate smoothed loss\n",
    "            if avg_loss is None:\n",
    "                avg_loss = loss\n",
    "            else:\n",
    "                avg_loss = smooth_beta * avg_loss + (1 - smooth_beta) * loss\n",
    "\n",
    "            # store values into logs\n",
    "            logs['lr'].append(lr)\n",
    "            logs['avg_loss'].append(avg_loss)\n",
    "            logs['loss'].append(loss)\n",
    "\n",
    "        logs.update({key: np.array(val) for key, val in logs.items()})\n",
    "        self.rollback_states()\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def cache_states(self):\n",
    "        cache_dict = {'model_state': deepcopy(self.model.state_dict()),\n",
    "                      'optimizer_state': deepcopy(self.optimizer.state_dict())}\n",
    "\n",
    "        return cache_dict\n",
    "\n",
    "    def rollback_states(self):\n",
    "        self.model.load_state_dict(self.cache['model_state'])\n",
    "        self.optimizer.load_state_dict(self.cache['optimizer_state'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Хуки\n",
    "В случае, если нет возможности использовать интерактивную отладку или добавить print, очень удобным может оказаться добавление forward/backward хуков: функций, которые сработают при вызове forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watches = {}\n",
    "def hook_fn(module, inp, out):\n",
    "    watches[module] = out.detach()\n",
    "\n",
    "for name, layer in nn._modules.items():\n",
    "    layer.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
