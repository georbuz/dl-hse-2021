{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIu1b5Xo4Bdd"
      },
      "source": [
        "# NN debug\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m12sl/dl-hse-2021/blob/master/04-debug/homework.ipynb)\n",
        "\n",
        "В этой тетрадке мы рассмотрим несколько проблем с обучением сеток и способы их решения.\n",
        "\n",
        "*Лучше решать эту домашку в колабе*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyTa9uFb4Bdl"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet18\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiLnuFXG4Bdm"
      },
      "source": [
        "# Data\n",
        "\n",
        "Для обучения сеток мы будем использовать MNIST.\n",
        "\n",
        "Качаем архив [Google Drive](https://drive.google.com/file/d/1xo-AIG2E6cTZbWGti1A5lp5FDtf4aHx_/view?usp=sharing). \n",
        "Его структура следующая:\n",
        "- /\n",
        "    - /train.csv\n",
        "    - /val.csv\n",
        "    - /train/{image_name}.png\n",
        "    - /val/{image_name}.png\n",
        "\n",
        "CSV файлы содержат название файла и его лейбл: image_name, label.\n",
        "\n",
        "Распакуйте архив в текущую папку:\n",
        "`unzip -q ./mnist_data2.zip -d ./`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA_WL0FlGZcO",
        "outputId": "42f84c6c-2ce3-4042-9760-f9cb613507e7"
      },
      "source": [
        "! gdown --id 1xo-AIG2E6cTZbWGti1A5lp5FDtf4aHx_\r\n",
        "! unzip -q ./mnist_data2.zip -d ./"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xo-AIG2E6cTZbWGti1A5lp5FDtf4aHx_\n",
            "To: /content/mnist_data2.zip\n",
            "36.8MB [00:00, 59.9MB/s]\n",
            "replace ./mnist_data/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: Т\n",
            "error:  invalid response [Т]\n",
            "replace ./mnist_data/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmvTQPTN4Bdo"
      },
      "source": [
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, images_dir_path: str,\n",
        "                 description_csv_path: str):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.images_dir_path = images_dir_path\n",
        "        self.description_df = pd.read_csv(description_csv_path,\n",
        "                                           dtype={'image_name': str, 'label': int})\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.description_df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img_name, label = self.description_df.iloc[index, :]\n",
        "        \n",
        "        img_path = Path(self.images_dir_path, f'{img_name}.png')\n",
        "        img = self._read_img(img_path)\n",
        "        \n",
        "        return dict(sample=img, label=label)\n",
        "    \n",
        "    @staticmethod\n",
        "    def _read_img(img_path: Path):\n",
        "        img = cv2.imread(str(img_path.resolve()))\n",
        "        img = img.astype(np.float32)\n",
        "        img = np.transpose(img, (2, 0, 1))\n",
        "        \n",
        "        return img"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8b_O3Y94Bdq"
      },
      "source": [
        "## Задание 1\n",
        "**(0.4 балла)** Запустите обучение сети в ячейках ниже. За 10 эпох метрика на валидации вырастает всего до ~0.15.\n",
        "\n",
        "*Вопросы:*\n",
        "1. Почему сетка так плохо учится?\n",
        "1. Найдите ошибку в коде и объясните ошибка вызывает подобное поведение в обучении?\n",
        "\n",
        "*Requirements:*\n",
        "1. Напишите ответы в markdown ячейке перед следующим заданием\n",
        "1. В следующей ячейке (после вашего ответа) вставьте код с исправлением ошибки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6TO3Gc1rD6t"
      },
      "source": [
        "class ResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = resnet18()\n",
        "        self.net.fc = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def compute_all(self, batch):\n",
        "        x = batch['sample'] / 255.0\n",
        "        y = batch['label']\n",
        "        logits = self.net(x)\n",
        "\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        acc = (logits.argmax(axis=1) == y).float().mean().cpu().numpy()\n",
        "        metrics = dict(acc=acc)\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model: nn.Module,\n",
        "                 optimizer,\n",
        "                 train_dataset: Dataset,\n",
        "                 val_dataset: Dataset,\n",
        "                 tboard_log_dir: str,\n",
        "                 batch_size: int = 128):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.device = 'cpu'\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.cuda.current_device()\n",
        "            self.model = self.model.to(self.device)\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.log_writer = SummaryWriter(log_dir=tboard_log_dir)\n",
        "\n",
        "    def train(self, num_epochs: int):\n",
        "        model = self.model\n",
        "        optimizer = self.optimizer\n",
        "\n",
        "        train_loader = DataLoader(self.train_dataset, shuffle=False, batch_size=self.batch_size)\n",
        "        val_loader = DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size)\n",
        "        best_loss = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for batch in tqdm(train_loader):\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                loss, details = model.compute_all(batch)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                for k, v in details.items():\n",
        "                    self.log_writer.add_scalar(k, v, global_step=self.global_step)\n",
        "                self.global_step += 1\n",
        "\n",
        "            model.eval()\n",
        "            val_losses, val_metrics_list = [], []\n",
        "            for batch in tqdm(val_loader):\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                loss, details = model.compute_all(batch)\n",
        "                val_losses.append(loss.item())\n",
        "                val_metrics_list.append(details['acc'].item())\n",
        "\n",
        "            val_loss, val_metrics = np.mean(val_losses), np.mean(val_metrics_list)\n",
        "            self.log_writer.add_scalar('val/loss', val_loss, global_step=self.global_step)\n",
        "            self.log_writer.add_scalar('val/metrics', val_metrics, global_step=self.global_step)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ0jh7Te4Bdq"
      },
      "source": [
        "mnist_train = MNISTDataset(images_dir_path='./mnist_data/train/',\n",
        "                           description_csv_path='./mnist_data/train.csv')\n",
        "mnist_val = MNISTDataset(images_dir_path='./mnist_data/val/',\n",
        "                         description_csv_path='./mnist_data/val.csv')\n",
        "\n",
        "model = ResNet18()\n",
        "opt = optim.SGD(model.parameters(), lr=1e-2)\n",
        "\n",
        "trainer = Trainer(model=model, optimizer=opt, train_dataset=mnist_train,\n",
        "                  val_dataset=mnist_val, tboard_log_dir='./tboard_logs/exp1/')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxYrh8Ky4Bdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ac55cb-e1d7-4781-eeb4-25721bf449f5"
      },
      "source": [
        "trainer.train(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:25<00:00, 18.40it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.77it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.68it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.66it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.80it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.75it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.48it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.19it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.80it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.75it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.91it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.12it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 19.05it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.16it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 19.06it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.21it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.62it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.06it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.83it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lsEvSVk_p72"
      },
      "source": [
        "# %tensorboard --logdir ./tboard_logs/"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmCmctLLtbE4"
      },
      "source": [
        "DataLoader для обучения вызывается с параметром shuffle = False. \r\n",
        "\r\n",
        "Из-за того, что мы не перетасовываем данные на каждой эпохе, у нас получаются одинаковые мини-батчи, плюс есть вероятность, что в них попадают объекты одного класса.\r\n",
        "\r\n",
        "В общем, это приводит к сильному переобучению, что видно на графике выше.\r\n",
        "\r\n",
        "Надо просто поменять параметр на True и все будет ОК."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNGYzxs8tQIo"
      },
      "source": [
        "class Trainer_fixed(Trainer):\r\n",
        "    def train(self, num_epochs: int):\r\n",
        "        model = self.model\r\n",
        "        optimizer = self.optimizer\r\n",
        "\r\n",
        "        ###################### делаем shuffle=True #########################\r\n",
        "        train_loader = DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\r\n",
        "        ####################################################################\r\n",
        "        val_loader = DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size)\r\n",
        "        best_loss = float('inf')\r\n",
        "\r\n",
        "        for epoch in range(num_epochs):\r\n",
        "            model.train()\r\n",
        "            for batch in tqdm(train_loader):\r\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\r\n",
        "                loss, details = model.compute_all(batch)\r\n",
        "\r\n",
        "                optimizer.zero_grad()\r\n",
        "                loss.backward()\r\n",
        "                optimizer.step()\r\n",
        "\r\n",
        "                for k, v in details.items():\r\n",
        "                    self.log_writer.add_scalar(k, v, global_step=self.global_step)\r\n",
        "                self.global_step += 1\r\n",
        "\r\n",
        "            model.eval()\r\n",
        "            val_losses, val_metrics_list = [], []\r\n",
        "            for batch in tqdm(val_loader):\r\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\r\n",
        "                loss, details = model.compute_all(batch)\r\n",
        "                val_losses.append(loss.item())\r\n",
        "                val_metrics_list.append(details['acc'].item())\r\n",
        "\r\n",
        "            val_loss, val_metrics = np.mean(val_losses), np.mean(val_metrics_list)\r\n",
        "            self.log_writer.add_scalar('val/loss', val_loss, global_step=self.global_step)\r\n",
        "            self.log_writer.add_scalar('val/metrics', val_metrics, global_step=self.global_step)    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK8oM9nAwBEK"
      },
      "source": [
        "model = ResNet18()\r\n",
        "opt = optim.SGD(model.parameters(), lr=1e-2)\r\n",
        "\r\n",
        "trainer = Trainer_fixed(model=model, optimizer=opt, train_dataset=mnist_train,\r\n",
        "                  val_dataset=mnist_val, tboard_log_dir='./tboard_logs/exp1_fixed/')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9AXOE2zwJ9S",
        "outputId": "a2d124ed-491e-46e2-d408-661de40f2f74"
      },
      "source": [
        "trainer.train(10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:24<00:00, 18.80it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.27it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.66it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.98it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.92it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.98it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.81it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.83it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.83it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.21it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.84it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.01it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.86it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.31it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.79it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.56it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.91it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.96it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.64it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.52it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyZV3AH9wQpH"
      },
      "source": [
        "# %tensorboard --logdir ./tboard_logs/"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxac_l_74Bds"
      },
      "source": [
        "## Задание 2\n",
        "**(0.2 балла)** Запустите обучение сети в ячейках ниже. За 10 эпох сетка не покажет качества выше случайного угадывания.\n",
        "\n",
        "*Вопросы:*\n",
        "1. Почему сетка так плохо учится?\n",
        "1. Найдите ошибку в коде и объясните почему найденная ошибка вызывает подобное поведение в обучении?\n",
        "\n",
        "*Requirements:*\n",
        "1. Напишите ответы в markdown ячейке перед следующим заданием\n",
        "1. В следующей ячейке (после вашего ответа) вставьте код с исправлением ошибки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfyrLn8qrMOS"
      },
      "source": [
        "class ResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = resnet18()\n",
        "        self.net.fc = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def compute_all(self, batch):\n",
        "        x = batch['sample'] / 255.0\n",
        "        y = batch['label']\n",
        "        logits = self.net(x)\n",
        "\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        acc = (logits.argmax(axis=1) == y).float().mean().cpu().numpy()\n",
        "        metrics = dict(acc=acc)\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model: nn.Module,\n",
        "                 optimizer,\n",
        "                 train_dataset: Dataset,\n",
        "                 val_dataset: Dataset,\n",
        "                 tboard_log_dir: str,\n",
        "                 batch_size: int = 128):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.device = 'cpu'\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.cuda.current_device()\n",
        "            self.model = self.model.to(self.device)\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.log_writer = SummaryWriter(log_dir=tboard_log_dir)\n",
        "\n",
        "    def train(self, num_epochs: int):\n",
        "        model = self.model\n",
        "        optimizer = self.optimizer\n",
        "\n",
        "        train_loader = DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\n",
        "        val_loader = DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size)\n",
        "        best_loss = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for batch in tqdm(train_loader):\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                loss, details = model.compute_all(batch)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                for k, v in details.items():\n",
        "                    self.log_writer.add_scalar(k, v, global_step=self.global_step)\n",
        "                self.global_step += 1\n",
        "\n",
        "            model.eval()\n",
        "            val_losses, val_metrics_list = [], []\n",
        "            for batch in tqdm(val_loader):\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                loss, details = model.compute_all(batch)\n",
        "                val_losses.append(loss.item())\n",
        "                val_metrics_list.append(details['acc'].item())\n",
        "\n",
        "            val_loss, val_metrics = np.mean(val_losses), np.mean(val_metrics_list)\n",
        "            self.log_writer.add_scalar('val/loss', val_loss, global_step=self.global_step)\n",
        "            self.log_writer.add_scalar('val/metrics', val_metrics, global_step=self.global_step)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTsIwDwF4Bds"
      },
      "source": [
        "mnist_train = MNISTDataset(images_dir_path='./mnist_data/train/',\n",
        "                           description_csv_path='./mnist_data/train.csv')\n",
        "mnist_val = MNISTDataset(images_dir_path='./mnist_data/val/',\n",
        "                         description_csv_path='./mnist_data/val.csv')\n",
        "\n",
        "model = ResNet18()\n",
        "opt = optim.SGD(model.parameters(), lr=10e-2, weight_decay=9e-1)\n",
        "\n",
        "trainer = Trainer(model=model, optimizer=opt, train_dataset=mnist_train,\n",
        "                  val_dataset=mnist_val, tboard_log_dir='./tboard_logs/exp2')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvpEK2ornFb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a232a8-5eae-4058-cf37-5325203fbbb4"
      },
      "source": [
        "trainer.train(10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:25<00:00, 18.69it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.91it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.81it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.01it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.68it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.17it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.80it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.46it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.42it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.64it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.84it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.40it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 19.06it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.83it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.90it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.20it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.86it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.29it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.82it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.10it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAWnZdpcp7hE"
      },
      "source": [
        "# %tensorboard --logdir ./tboard_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbMLJEj5SErA"
      },
      "source": [
        "Слишком большой LR и WD. Скорее даже проблема именно в WD, вполучаются очень маленькие веса, в целом модель недообучается.\r\n",
        "\r\n",
        "Можно просто убрать регуляризацию, и на один порядок уменьшить LR - будет лучше"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z46PYD2CSFtd"
      },
      "source": [
        "model = ResNet18()\r\n",
        "opt = optim.SGD(model.parameters(), lr=1e-2, weight_decay=1e-1)\r\n",
        "\r\n",
        "trainer = Trainer(model=model, optimizer=opt, train_dataset=mnist_train,\r\n",
        "                  val_dataset=mnist_val, tboard_log_dir='./tboard_logs/exp2_fixed')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXlfVLKySKhE",
        "outputId": "4a1153b2-16d4-49be-8e25-4183a979377e"
      },
      "source": [
        "trainer.train(10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:25<00:00, 18.41it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.01it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.49it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.90it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.58it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.30it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.51it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.26it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.60it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 20.95it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.62it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.21it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.70it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.50it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.79it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.19it/s]\n",
            "100%|██████████| 469/469 [00:25<00:00, 18.74it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.42it/s]\n",
            "100%|██████████| 469/469 [00:24<00:00, 18.78it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOY7O4tmSNn8"
      },
      "source": [
        "# %tensorboard --logdir ./tboard_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn7r5QBD4Bds"
      },
      "source": [
        "## Задание 3\n",
        "**(0.4 балла)** Запустите обучение сети в ячейках ниже. В сети будут использоваться предобученные параметры, которые должны были помочь выдавать качество около 1. Однако, за 5 эпох сетка не выдаст качество, которое мы ожидали.\n",
        "\n",
        "Перед запуском ячеек скачайте используемое состояние модели [pretrained_model.pt](https://drive.google.com/file/d/1JITAz1L8mWpTGany84YMYKIhzVgsBf_9/view?usp=sharing).\n",
        "\n",
        "*Вопросы:*\n",
        "1. Почему сетка так плохо учится?\n",
        "1. Найдите ошибку и объясните почему найденная ошибка вызывает подобное поведение в обучении?\n",
        "\n",
        "*Requirements:*\n",
        "1. Напишите ответы в markdown ячейке после ячейки с тензорбордом.\n",
        "1. В следующей ячейке (после вашего ответа) вставьте код с исправлением ошибки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lhViPcNUtol",
        "outputId": "ea5d179d-3118-44b6-96bf-e281b04a4af0"
      },
      "source": [
        "! gdown --id 1JITAz1L8mWpTGany84YMYKIhzVgsBf_9"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JITAz1L8mWpTGany84YMYKIhzVgsBf_9\n",
            "To: /content/pretrained_model.pt\n",
            "44.8MB [00:00, 123MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibwv4nXw4Bds"
      },
      "source": [
        "class ResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = resnet18()\n",
        "        self.net.fc = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def compute_all(self, batch):\n",
        "        x = batch['sample'] / 255.0\n",
        "        y = batch['label']\n",
        "        logits = self.net(x)\n",
        "\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        acc = (logits.argmax(axis=1) == y).float().mean().cpu().numpy()\n",
        "        metrics = dict(acc=acc)\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model: nn.Module,\n",
        "                 optimizer,\n",
        "                 train_dataset: Dataset,\n",
        "                 val_dataset: Dataset,\n",
        "                 tboard_log_dir: str,\n",
        "                 batch_size: int = 128):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.device = 'cpu'\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.cuda.current_device()\n",
        "            self.model = self.model.to(self.device)\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.log_writer = SummaryWriter(log_dir=tboard_log_dir)\n",
        "\n",
        "    def train(self, num_epochs: int):\n",
        "        model = self.model\n",
        "        optimizer = self.optimizer\n",
        "\n",
        "        train_loader = DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\n",
        "        val_loader = DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size)\n",
        "        best_loss = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for batch in tqdm(train_loader):\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                loss, details = model.compute_all(batch)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                for k, v in details.items():\n",
        "                    self.log_writer.add_scalar(k, v, global_step=self.global_step)\n",
        "                self.global_step += 1\n",
        "\n",
        "            model.eval()\n",
        "            val_losses, val_metrics_list = [], []\n",
        "            for batch in tqdm(val_loader):\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                loss, details = model.compute_all(batch)\n",
        "                val_losses.append(loss.item())\n",
        "                val_metrics_list.append(details['acc'].item())\n",
        "\n",
        "            val_loss, val_metrics = np.mean(val_losses), np.mean(val_metrics_list)\n",
        "            self.log_writer.add_scalar('val/loss', val_loss, global_step=self.global_step)\n",
        "            self.log_writer.add_scalar('val/metrics', val_metrics, global_step=self.global_step)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5kMzqkFK3qE"
      },
      "source": [
        "mnist_train = MNISTDataset(images_dir_path='./mnist_data/train/',\n",
        "                           description_csv_path='./mnist_data/train.csv')\n",
        "mnist_val = MNISTDataset(images_dir_path='./mnist_data/val/',\n",
        "                         description_csv_path='./mnist_data/val.csv')\n",
        "\n",
        "model = ResNet18()\n",
        "model_sate_path = 'pretrained_model.pt'\n",
        "model.load_state_dict(torch.load(model_sate_path, map_location='cpu'))\n",
        "\n",
        "opt = optim.SGD(model.parameters(), lr=1e-2)\n",
        "\n",
        "trainer = Trainer(model=model, optimizer=opt, train_dataset=mnist_train,\n",
        "                  val_dataset=mnist_val, tboard_log_dir='./tboard_logs/exp3')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQBMJL3JK3v9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "37cc5ca2-1386-425e-adfa-cf3d16a34bd7"
      },
      "source": [
        "trainer.train(5)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:24<00:00, 19.06it/s]\n",
            "100%|██████████| 79/79 [00:03<00:00, 21.37it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-4fffd586b790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-8b0ebae9949f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    110\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX7EJujjLg_X"
      },
      "source": [
        "# %tensorboard --logdir ./tboard_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-2iAhSVl6Ze"
      },
      "source": [
        "Судя по тому, что модель в принципе не обучается - есть проблема c весами и градиентами. Попробуем посмотреть результат forward pass'а на искуственных сэмплах"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLBAzCyD7snB"
      },
      "source": [
        "c, w, h = mnist_train.__getitem__(0)[\"sample\"].shape\r\n",
        "\r\n",
        "x1 = torch.rand((1, c, w, h))\r\n",
        "x2 = torch.ones((1, c, w, h))\r\n",
        "x3 = torch.zeros((1, c, w, h))\r\n",
        "\r\n",
        "out1 = model(x1.to(0))\r\n",
        "out2 = model(x2.to(0))\r\n",
        "out3 = model(x3.to(0))\r\n",
        "\r\n",
        "print(torch.all(out1 == out2))\r\n",
        "print(torch.all(out2 == out3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_M_CGQ191Pw"
      },
      "source": [
        "Результат один. Скорее всего где-то есть слои с нулевыми весами, найдем их"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUYPqo8I99AC"
      },
      "source": [
        "zeros_layers = []\r\n",
        "\r\n",
        "for name, param in model.named_parameters():\r\n",
        "    if torch.all(param == 0):\r\n",
        "        zeros_layers.append(name)\r\n",
        "\r\n",
        "print(zeros_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plDh2sSY-cZU"
      },
      "source": [
        "__Ответ__: действительно, некоторые веса равны нулю, из-за чего и возникает проблема, получается, что входные фичи в какой-то момент forward pass'a зануляется, т.е. не учитываются, а из-за этого на выходе модели получается один и тот же вектор для всех сэмплов.\r\n",
        "\r\n",
        "Исправить это можно просто переинициализировав веса в этих слоях"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6KxK4MzpCe9"
      },
      "source": [
        "mnist_train = MNISTDataset(images_dir_path='./mnist_data/train/',\r\n",
        "                           description_csv_path='./mnist_data/train.csv')\r\n",
        "mnist_val = MNISTDataset(images_dir_path='./mnist_data/val/',\r\n",
        "                         description_csv_path='./mnist_data/val.csv')\r\n",
        "\r\n",
        "model = ResNet18()\r\n",
        "model_sate_path = 'pretrained_model.pt'\r\n",
        "weights = torch.load(model_sate_path, map_location='cpu')\r\n",
        "\r\n",
        "zeros_layers = []\r\n",
        "\r\n",
        "# пробегаемся по предобученным весам, ищем полностью нулевые слови\r\n",
        "for name, param in weights.items():\r\n",
        "    if torch.all(param == 0):\r\n",
        "        zeros_layers.append(name)\r\n",
        "\r\n",
        "# переинициализируем их, например, равномерно\r\n",
        "for layer in zeros_layers:\r\n",
        "    nn.init.uniform_(weights[layer])\r\n",
        "\r\n",
        "\r\n",
        "model.load_state_dict(weights)\r\n",
        "\r\n",
        "opt = optim.SGD(model.parameters(), lr=1e-2)\r\n",
        "\r\n",
        "trainer = Trainer(model=model, optimizer=opt, train_dataset=mnist_train,\r\n",
        "                  val_dataset=mnist_val, tboard_log_dir='./tboard_logs/exp3_fixed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAE7HqkosZaE"
      },
      "source": [
        "trainer.train(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70A5jUs0dDNi"
      },
      "source": [
        "# %tensorboard --logdir ./tboard_logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}